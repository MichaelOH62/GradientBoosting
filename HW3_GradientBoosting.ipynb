{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Michael O'Hanlon\\\n",
        "Professor Monogioudis\\\n",
        "CS301101\\\n",
        "11/13/2022\n",
        "\n",
        "Assignment #3: Electromyography and Gradient Boosting"
      ],
      "metadata": {
        "id": "IYhOcDddpPjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background and Information\n",
        "\n",
        "Gradient Boosting is a very popular and effective machine learning effective. It works to combine several weak models or learners into a strong model, where a weak model is defined as one with poor accuracy performance. If a model struggles to perform better than complete random predictions, it can be classified as a weak model. So, the motivation behind using Gradient Boosting to take several models with poor accuracy performance and combine them into a single strong model that has good accuracy, which if what we want of course!\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "What follows is the four steps of the Gradient Boosting Algorithm.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "The Gradient Boosting Algorithm:\\\n",
        "![](https://drive.google.com/uc?export=view&id=1-4Ai4qRnqSM6I74AYL9qPcihuX9cLGct)\n",
        "\n",
        "When M is sufficiently large, the result is a strong composite model which can be used to make predictions.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "For the classification task, the loss function is defined as:\\\n",
        "![](https://drive.google.com/uc?export=view&id=1vlSKPN42rTilwlieC9bwZjbprS14LeZM)\n",
        "\n",
        "The trick behind Gradient Boosting is to fit a new model to the *residual errors* that are created by the previous model. As expected from it's name, Gradient Boosting utilizes calculating the gradient for every step. This is used to update the weights to ensure we are actually heading in the right direction.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "An important component of Gradient Boosting is to finely tune the two hyperparameters, `learning_rate` and `n_estimators`. The first hyperparamter, `learning_rate`, is used to provide a weight to how much each weak learner actually contributes. The second, `n_estimators`, is used to give a number to the amount of models in the ensemble. These two hyperparameters must be tuned to prevent overfitting and ensure the resultant model is high in accuracy.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "In this notebook, Gradient Boosting will be implemented from scratch using JAX libraries. The implementation will then be applied to a Electromyography dataset to see it's performance in action."
      ],
      "metadata": {
        "id": "A22QX8yEpVh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding from scratch using JAX"
      ],
      "metadata": {
        "id": "ScVOf9R_pbG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMG Dataset"
      ],
      "metadata": {
        "id": "hRLDeKfCphXR"
      }
    }
  ]
}