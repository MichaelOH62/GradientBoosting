{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Michael O'Hanlon\\\n",
        "Professor Monogioudis\\\n",
        "CS301101\\\n",
        "11/17/2022\n",
        "\n",
        "Assignment #3: Electromyography and Gradient Boosting"
      ],
      "metadata": {
        "id": "IYhOcDddpPjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background and Information\n",
        "\n",
        "Gradient Boosting is a very popular and effective machine learning effective. It works to combine several weak models or learners into a strong model, where a weak model is defined as one with poor accuracy performance. If a model struggles to perform better than complete random predictions, it can be classified as a weak model. So, the motivation behind using Gradient Boosting to take several models with poor accuracy performance and combine them into a single strong model that has good accuracy, which if what we want of course!\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "What follows is the four steps of the Gradient Boosting Algorithm.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "The Gradient Boosting Algorithm:\\\n",
        "![](https://drive.google.com/uc?export=view&id=1-4Ai4qRnqSM6I74AYL9qPcihuX9cLGct)\n",
        "\n",
        "When M is sufficiently large, the result is a strong composite model which can be used to make predictions.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "For the classification task, the loss function is defined as:\\\n",
        "![](https://drive.google.com/uc?export=view&id=1MIuSSvHCDbI2H7IH3H5ru-FkVu2rYAGi)\n",
        "\n",
        "However, in the case of multiclass classification, which will be applied later, the loss function is defined as:\\\n",
        "![](https://drive.google.com/uc?export=view&id=10T9G2VYHVJR-CgvRmb4kRAROfkfvV4U-)\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "The trick behind Gradient Boosting is to fit a new model to the *residual errors* that are created by the previous model. As expected from it's name, Gradient Boosting utilizes calculating the gradient for every step. This is used to update the weights to ensure we are actually heading in the right direction.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "An important component of Gradient Boosting is to finely tune the two hyperparameters, `learning_rate` and `n_estimators`. The first hyperparameter, `learning_rate`, is used to provide a weight to how much each weak learner actually contributes. The second, `n_estimators`, is used to give a number to the amount of models in the ensemble. These two hyperparameters must be tuned to prevent overfitting and ensure the resultant model is high in accuracy.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "In this notebook, Gradient Boosting will be implemented from scratch using JAX libraries. The implementation will then be applied to a Electromyography dataset to see it's performance in action."
      ],
      "metadata": {
        "id": "A22QX8yEpVh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding from scratch using JAX"
      ],
      "metadata": {
        "id": "ScVOf9R_pbG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implementation of Gradient Boosting from scratch.\n",
        "Utilizes JAX libraries to perform classification.\n",
        "\n",
        "Video Reference: \n",
        "https://www.youtube.com/watch?v=SstuvS-tVc0&list=WL&index=2&t=7s&ab_channel=AleksaGordi%C4%87-TheAIEpiphany\n",
        "\n",
        "Useful Links:\n",
        "https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d\n",
        "https://www.simplilearn.com/gradient-boosting-algorithm-in-python-article\n",
        "https://github.com/groverpr/Machine-Learning/blob/master/notebooks/01_Gradient_Boosting_Scratch.ipynb\n",
        "\n",
        "https://gkaissis.github.io/post/2020-03-15-rfgb/\n",
        "https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/gradient_boosting.py\n",
        "\"\"\"\n",
        "\n",
        "#Necessary imports\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#Loss function for classification task\n",
        "def CrossEntropy(y_true:jnp.array, y_proba:jnp.array):\n",
        "    y_proba = jnp.clip(y_proba, 1e-5, 1 - 1e-5)\n",
        "    return jnp.sum(- y_true * jnp.log(y_proba) - (1 - y_true) * jnp.log(1 - y_proba))\n",
        "\n",
        "#Class for Gradient Boosting\n",
        "class GradientBooster:\n",
        "    #Construct a new GradientBooster\n",
        "    def __init__(self, n_estimators, learning_rate, **kwargs):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = CrossEntropy\n",
        "\n",
        "        #Create all of the estimators to use together\n",
        "        self.estimators = []\n",
        "        for _ in range(self.n_estimators):\n",
        "                self.estimators.append(DecisionTreeRegressor(**kwargs))\n",
        "\n",
        "    #Function to train the classifier with given Xs and ys\n",
        "    def fit(self, X:np.array, y:np.array):\n",
        "        y_pred = np.full(np.shape(y), np.mean(y))\n",
        "        for i, estimator in enumerate(self.estimators):\n",
        "            gradient = jax.grad(self.loss, argnums=1)(y.astype(np.float32), y_pred.astype(np.float32))\n",
        "            self.estimators[i].fit(X, gradient)\n",
        "            update = self.estimators[i].predict(X)\n",
        "            y_pred -= (self.learning_rate * update)\n",
        "\n",
        "    #Function to make predictions based on X data\n",
        "    def predict(self, X:np.array):\n",
        "        y_pred = np.zeros(X.shape[0], dtype=np.float32)\n",
        "        for estimator in self.estimators:\n",
        "            y_pred -= (self.learning_rate * estimator.predict(X))\n",
        "\n",
        "        #Return the prediction\n",
        "        return np.where(1/(1 + np.exp(-y_pred))>.5, 1, 0)"
      ],
      "metadata": {
        "id": "XrCKGPsOFdIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMG Dataset"
      ],
      "metadata": {
        "id": "hRLDeKfCphXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the EMG dataset into the Colab environment.\n",
        "Apply the Gradient Boosting implementation to the model.\n",
        "\n",
        "20 Classes (10 Aggressive, 10 Normal)\n",
        "Load the data into a csv and put in Google Drive\n",
        "Add a column to the data indicating it's class\n",
        "Load the data from all 4 sub folders\n",
        "Seperate the data into X (features) and y (class)\n",
        "Split the data into training and test\n",
        "\"\"\"\n",
        "\n",
        "#Necessary imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Create a gradient booster that creates 100 estimators with a learning rate of 0.1\n",
        "gradBooster = GradientBooster(100, 0.1)"
      ],
      "metadata": {
        "id": "vY7ABiWbFYbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}